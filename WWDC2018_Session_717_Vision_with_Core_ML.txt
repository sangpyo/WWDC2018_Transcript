안녕하세요. WWDC에 오신 것을 환영합니다.
 나는 내가 커피와 쿠키와 경쟁하고 있다는 것을 알고 있지만 커피가 글루텐이 없는지, 쿠키가 실제로 카페인이 없는지 여부를 모르겠다.
 제 이름은 Frank Doepke입니다. Core ML과 Division Framework를 사용하여 컴퓨터 비전으로 할 수있는 흥미로운 것들에 대해 이야기 할 것입니다.
 그래서, 오늘 우리는 무엇에 관해 이야기 할 것입니까? 첫째, 맞춤 이미지 분류 측면에서 우리에게 특별한 것이 있다고 들었을 것입니다.
 그런 다음 우리는 약간의 물체 인식을 할 것입니다.
 그리고 마지막으로 중요한 것은 몇 가지 기본 사항에 뛰어 들어 비전 인지도를 높이겠습니다.
 이제 사용자 지정 이미지 분류를 통해 이전에 일부 프레젠테이션에서 이미 장점을 확인했으며 꽃과 과일로 할 수있는 것처럼 보였습니다. 다른 모든 사람들처럼 꽃과 과일도 좋아합니다.
 미안, 리지,하지만 우리는 여기서 좀 더 기술적 인 것을 할 수있을 거라 생각했다.
 그래서, 아이디어는, 우리가 상점을 만들고 우리 모두가 괴짜라면, 우리가 로봇을 만들 수있는 가게를 짓도록하자.
 따라서 우리가 알아야 할 부분이 있습니다.
 실제로 내 상점의 고객이 이러한 객체가 무엇인지 식별 ​​할 수있는 앱이 있으면 멋질 것이라고 생각했습니다.
 그래서, 우리는 고객 분류자를 훈련시켜야합니다.
 그런 다음 분류 기준이 있으면 모든 기기에서 실제로 실행할 수있는 iOS 앱을 만듭니다.
 그리고이 과정을 밟을 때, 저는 여러분이 실제로 이런 것들을하고 그것을 통해 여러분을 인도하려고 할 때 일반적인 함정을 빠져 나갈 것입니다.
 우리의 훈련부터 시작합시다.
 그래서 우리는 어떻게 훈련합니까? 우리는 물론 ML을 사용합니다.
 물론 우리가해야 할 첫 번째 단계는 사진을 찍어야한다는 것입니다.
 그런 다음 폴더에 넣고 폴더 이름을 분류 레이블로 사용합니다.
 이제는 모든 사람들이 가지고있는 가장 큰 질문 인 "얼마나 많은 데이터가 필요합니까?" 첫 번째로, 카테고리 당 최소 약 10 개의 이미지가 필요하지만, 이는 저조한 측면에 있습니다.
 당신은 분명히 더 갖고 싶습니다.
 실제로 분류 기준을 높일수록 실적이 향상됩니다.
 또 하나 주목해야 할 것은 매우 불균형 한 데이터 세트입니다.
 그게 무슨 뜻이야? 하나의 카테고리에 수천 개의 이미지가 있고 다른 이미지에 10 개의 이미지 만있는 데이터 세트는이 모델이 실제로 잘 훈련되지 않습니다.
 따라서 대부분의 카테고리간에 동등한 분배가 이루어지기를 원합니다.
 우리가 실제로 소개하는 또 하나의 기능은 기능 보강입니다.
 Augmentation은이 모델을보다 강력하게 만들 수 있지만 실제로는 다양성을 대체하지는 못합니다.
 따라서, 당신은 여전히 ​​당신이 분류하기를 원하는 당신의 사물들의 많은 이미지들을 갖고 싶습니다.
 그러나 증강과 함께, 우리가 할 일은 이미지를 취하고 그것을 교란시키는 것입니다.
 그래서, 우리는 그것을 천천히, 우리는 그것을 흐리게하고, 회전시키고, 뒤집어서, 실제로 우리가 그것을 훈련 할 때 분류 자와 다르게 보입니다.
 우리 교육이 실제로 어떻게 작동하는지 조금 살펴 보겠습니다.
 당신은 이미 그것을 들었을 것입니다, 용어, 전수 학습.
 이것이 우리가 분류자를 훈련 할 때 ML 생성에서 사용할 것입니다.
 그래서, 우리는 미리 훈련 된 모델로 시작합니다. 그리고 그것이 모든 무거운 짐들이 실제로 일어나는 곳입니다.
 이 모델은 정상적으로 몇 주 동안 그리고 수백만 개의 이미지로 훈련되며,이 작업은 실제로이 작업을 수행하는 데 필요한 첫 번째 출발점입니다.
 이 모델에서 이것을 피쳐 추출기로 사용할 수 있습니다.
 이것은 우리에게 우리가 가지고있는 이미지에 대한 수치적인 설명 인 특징 벡터를 제공합니다.
 이제 데이터를 가져 와서 세트를 교육합니다. 라벨 데이터에 실제 분류 기준 인 마지막 레이어라고 부르며 사용자 정의 모델이 나옵니다.
 자, 나는이 커다란 첫 번째 pretrained 모델을 이미 언급했다.
 그리고 우리는이 비전 [비전]에 새로운 것을 가지고 있습니다.
 이것이 우리가 Scene FeaturePrint for Scene이라고 부르는 것입니다.
 ML 작성을 통해 사용할 수 있으며 이미지 분류자를 학습시킬 수 있습니다.
 매우 큰 데이터 세트에 대한 교육을 받았으며 1,000 개가 넘는 카테고리를 분류 할 수 있습니다.
 이것은 실제로 사용할 수있는 꽤 좋은 배포판입니다.
 그리고 우리는 이미 그것을 사용했습니다.
 지난 몇 년 동안 사진에서 보았던 사용자 [들리지 않음] 사진 중 일부는 실제로이 모델을 아래에서 사용했습니다.
 우리는 또한 그 모델을 지속적으로 향상시킬 것입니다. 그러나 여기서 제가 강조하고자하는 작은 경고가 있습니다.
 우리가 그 모델의 새로운 버전을 발표 할 때, 당신은 새로운 모델을 재 훈련하지 않으면 반드시 이점을 자동으로 얻지 못할 것입니다.
 따라서 금년에 개발을 시작하면 금년에 우리가 다음 해에 무엇이 나왔는지를 알고 데이터 세트를 잡고 실제로 들릴 수 있습니다.
 Google의 기능 추출기에 대한 몇 가지 추가 정보
 이미 장치에 장착되어있어 모델에 대한 디스크 공간을 상당히 줄일 수 있기 때문에 중요한 결정이었습니다.

자, 조금 비교합시다.
 그래서 저는 오늘날 우리가 사용하는 상용 모델을 선택했습니다.
 첫 번째는 Resnet입니다.
 그래서 Resnet 위에 분류기를 훈련하면 모델이 얼마나 큰가요? 98 메가 바이트.
 Squeezenet을 사용하면 Squeezenet은 훨씬 작은 모델입니다.
 차별화 할 수는 없지만 5 메가 바이트입니다.
 그래서, 거기에 저축하는 것이지만, 그렇게 다양하지는 않을 것입니다.
 이제 Vision은 어떻습니까? 대부분의 경우 1 메가 바이트 미만입니다.
 물론 이것이 우리가 사용하기에 좋은 선택이라고 믿는 이유 중 하나는 이미 최적화 된 것입니다.
 우리는 하드웨어 또는 GPU와 CPUS에 대해 몇 가지 사실을 알고 있으며, 우리는이 모델에서 많은 것을 최적화하여 디바이스에서 최고 성능을 발휘했습니다.
 그럼 우리는 어떻게 [들리지 않음]을 훈련합니까? 우리는 라벨이 붙은 이미지로 시작하여 ML 만들기로 가져오고 ML은 [들리지 않음] Vision Feature Print를 추출하는 방법을 알고 있습니다.
 우리 분류기를 훈련 시키며 그 분류기는 핵심 ML 모델에 들어가는 모든 것입니다.
 그래서 그것이 너무 작습니다.
 이제 실제로 이미지를 분석하고 싶을 때, 이미지와 모델을 사용하면됩니다. Vision이나 Core ML에서, 다시 훈련 할 수있는 또 다른 방법을 안다. - 죄송합니다.
 훈련하지 마라.
 이 경우 Vision Feature Print를 사용하면 분류를 알 수 없습니다.
 그래서 그것은 우리가 훈련에 대해 알아야 할 모든 것입니다.
 그러나 우리가 앱을 다룰 때 당신이보기를 바라는 몇 가지주의 사항이 있다고 말했습니다.
 그래서, 우선, 우리는 실제로 우리가해야 할 때 우리의 분류자를 실제로 실행하기를 원합니다.
 분류 자들은 계산 집약적 인 심층 컨벌루션 네트워크입니다.
 그래서, 우리가 그것을 실행할 때, 여러분이 알고있는 종류의 전자와 CPU 및 GPU를 사용하게 될 것입니다.
 그래서, 당신이 정말로해야하지 않는 한 당신은 이것을 사용하고 싶지 않습니다.
 데모 래터럴에 보여줄 예제에서 실제로 카메라가 움직이는 순간이 아니라 실제로 사람이 실제로 물건을 보는지 만 분류하고 싶습니다.
 그래서, 나는 아직도 물어볼 것인가? "라고 묻습니다. 그런 다음 분류기를 실행할 것입니다.
 어떻게해야합니까? 비전을 사용하여 등록을 사용할 수 있습니다.
 등록이란 두 개의 이미지를 가져 와서 서로 정렬 할 수 있음을 의미합니다.
 그리고 여러분은 저에게 "알았습니다. 픽셀 크기만큼이 픽셀을 이동 시키면 실제로 어떻게 실제로 일치시킬 것인가에 있습니다.
"이것은 매우 저렴하고 빠른 알고리즘이며 카메라를 들거나 카메라 앞에서 움직이는 것이 있으면 알려줄 것입니다.
 나는 VN Translational Image Registration Request를 사용했다.
 나는 그것이 입이 다름을 압니다.
 그러나 그것은 저에게이 모든 정보를 줄 것입니다.
 그래서 이것을 먼저 시각화하기 위해 작은 비디오를 봅시다.
 이 비디오에서 내가하는 일은 근본적으로 약간의 노란 선을 보여 주며 기본적으로 내 카메라가 어떻게 움직 였는지 또는 등록 요청이 프레임의 마지막 몇 프레임으로 이동했는지를 보여줍니다.
 그래서, 노란색 선을보고 얼마가 지났는지를 확인한 다음 카메라를 꽤 많이 움직였습니다. 그리고 카메라를 잡고있을 때 실제로는 아주 작은 선이어야합니다.
 그래서 카메라가 움직이는 것을 보았습니다. 이제 저는 이것에 초점을 맞추고 있으며, 매우 짧아집니다.
 그래서, 그것은 좋은 생각입니다.
 그것은 좋아, "좋아, 이제 나는 여전히 잡고있다.
 이제 분류자를 실행하고 싶습니다.
"염두에 두어야 할 다음 사항은 백업 계획입니다.
 백업 계획을 세우는 것이 좋습니다.
 분류가 잘못 될 수 있습니다.
 그리고 그것이 의미하는 바는, 비록 나의 분류가 실제로 높은 자신감을 가지고 있다고해도, 때로는 그것이 올바르게 작동하지 않는다는 계획의 종류가 필요하다는 것입니다.
 당신이 나중에 보게 될 것처럼, 제가 여기서 한 예는 제가 물리적 인 대상이없는 것을 가지고 있습니다.
 그래서 어떻게 해결할 수 있을까요? 필자는 비전 프레임 워크에있는 후방 탐지기를 사용하여이를 식별하기 위해 일부 데이터 역방향 레이블을 읽으 려합니다.
 좋아, 슬라이드가 충분 해.
 누가 데모를보고 싶어? 좋아요, 화면의 오른쪽에 보이는 것은 내 장치입니다.
 지금은 작은 로봇 가게 응용 프로그램을 시작할 것입니다.
 그리고 내가 돌아 다니는 것을 볼 때 아무 일도 일어나지 않습니다.
 내가 잠시 멈추고 뭔가를 가리키면 노란 선이 보일 것입니다. 그리고 나서 짜잔이 나옵니다. 네 단계는 스테퍼 모터입니다.
 괜찮아? 우리가이 테이블에서 무엇을 더 가지고 있는지 보자. 저의 마이크로 컨트롤러입니다.
 그것은 스테퍼 모터 드라이버입니다.
 우리는 또한 당신이 알고있는 것을 좋아할 수 있습니다.
 예, 폐 루프 벨트입니다.
 우리가 여기 무엇을 가지고 있니? 리드 스크류.
 그리고 제가 말씀 드렸듯이, 제 케이블을 충분히 길게 들여다 보면 바코드를 볼 수 있습니다.
 그리고 그것이 나의 훈련 과정입니다.
 이봐, 프랭크? 물론, 그걸로 나는 프랭크? 무슨 일이야? Frank, 네, 데모에 다른 로봇 파트를 추가해야합니다.
 이쪽은 브렛입니다.
 내 매니저 야.
 그거 좋을거야.
 평소처럼 관리, 막판 요청.
 그 메모 사본 한 부를 더 얻도록하겠습니다.
 나는 이것을 위해 토요일에 가지 않을 것입니다.
 좋아, 그럼 우린 뭘 여기에 있니?

 자, 조금 비교합시다.
 그래서 저는 오늘날 우리가 사용하는 상용 모델을 선택했습니다.
 첫 번째는 Resnet입니다.
 그래서 Resnet 위에 분류기를 훈련하면 모델이 얼마나 큰가요? 98 메가 바이트.
 Squeezenet을 사용하면 Squeezenet은 훨씬 작은 모델입니다.
 차별화 할 수는 없지만 5 메가 바이트입니다.
 그래서, 거기에 저축하는 것이지만, 그렇게 다양하지는 않을 것입니다.
 이제 Vision은 어떻습니까? 대부분의 경우 1 메가 바이트 미만입니다.
 물론 이것이 우리가 사용하기에 좋은 선택이라고 믿는 이유 중 하나는 이미 최적화 된 것입니다.
 우리는 하드웨어 또는 GPU와 CPUS에 대해 몇 가지 사실을 알고 있으며, 우리는이 모델에서 많은 것을 최적화하여 디바이스에서 최고 성능을 발휘했습니다.
 그럼 우리는 어떻게 [들리지 않음]을 훈련합니까? 우리는 라벨이 붙은 이미지로 시작하여 ML 만들기로 가져오고 ML은 [들리지 않음] Vision Feature Print를 추출하는 방법을 알고 있습니다.
 우리 분류기를 훈련 시키며 그 분류기는 핵심 ML 모델에 들어가는 모든 것입니다.
 그래서 그것이 너무 작습니다.
 이제 실제로 이미지를 분석하고 싶을 때, 이미지와 모델을 사용하면됩니다. Vision이나 Core ML에서, 다시 훈련 할 수있는 또 다른 방법을 안다. - 죄송합니다.
 훈련하지 마라.
 이 경우 Vision Feature Print를 사용하면 분류를 알 수 없습니다.
 그래서 그것은 우리가 훈련에 대해 알아야 할 모든 것입니다.
 그러나 우리가 앱을 다룰 때 당신이보기를 바라는 몇 가지주의 사항이 있다고 말했습니다.
 그래서, 우선, 우리는 실제로 우리가해야 할 때 우리의 분류자를 실제로 실행하기를 원합니다.
 분류 자들은 계산 집약적 인 심층 컨벌루션 네트워크입니다.
 그래서, 우리가 그것을 실행할 때, 여러분이 알고있는 종류의 전자와 CPU 및 GPU를 사용하게 될 것입니다.
 그래서, 당신이 정말로해야하지 않는 한 당신은 이것을 사용하고 싶지 않습니다.
 데모 래터럴에 보여줄 예제에서 실제로 카메라가 움직이는 순간이 아니라 실제로 사람이 실제로 물건을 보는지 만 분류하고 싶습니다.
 그래서, 나는 아직도 물어볼 것인가? "라고 묻습니다. 그런 다음 분류기를 실행할 것입니다.
 어떻게해야합니까? 비전을 사용하여 등록을 사용할 수 있습니다.
 등록이란 두 개의 이미지를 가져 와서 서로 정렬 할 수 있음을 의미합니다.
 그리고 여러분은 저에게 "알았습니다. 픽셀 크기만큼이 픽셀을 이동 시키면 실제로 어떻게 실제로 일치시킬 것인가에 있습니다.
"이것은 매우 저렴하고 빠른 알고리즘이며 카메라를 들거나 카메라 앞에서 움직이는 것이 있으면 알려줄 것입니다.
 나는 VN Translational Image Registration Request를 사용했다.
 나는 그것이 입이 다름을 압니다.
 그러나 그것은 저에게이 모든 정보를 줄 것입니다.
 그래서 이것을 먼저 시각화하기 위해 작은 비디오를 봅시다.
 이 비디오에서 내가하는 일은 근본적으로 약간의 노란 선을 보여 주며 기본적으로 내 카메라가 어떻게 움직 였는지 또는 등록 요청이 프레임의 마지막 몇 프레임으로 이동했는지를 보여줍니다.
 그래서, 노란색 선을보고 얼마가 지났는지를 확인한 다음 카메라를 꽤 많이 움직였습니다. 그리고 카메라를 잡고있을 때 실제로는 아주 작은 선이어야합니다.
 그래서 카메라가 움직이는 것을 보았습니다. 이제 저는 이것에 초점을 맞추고 있으며, 매우 짧아집니다.
 그래서, 그것은 좋은 생각입니다.
 그것은 좋아, "좋아, 이제 나는 여전히 잡고있다.
 이제 분류자를 실행하고 싶습니다.
"염두에 두어야 할 다음 사항은 백업 계획입니다.
 백업 계획을 세우는 것이 좋습니다.
 분류가 잘못 될 수 있습니다.
 그리고 그것이 의미하는 바는, 비록 나의 분류가 실제로 높은 자신감을 가지고 있다고해도, 때로는 그것이 올바르게 작동하지 않는다는 계획의 종류가 필요하다는 것입니다.
 당신이 나중에 보게 될 것처럼, 제가 여기서 한 예는 제가 물리적 인 대상이없는 것을 가지고 있습니다.
 그래서 어떻게 해결할 수 있을까요? 필자는 비전 프레임 워크에있는 후방 탐지기를 사용하여이를 식별하기 위해 일부 데이터 역방향 레이블을 읽으 려합니다.
 좋아, 슬라이드가 충분 해.
 누가 데모를보고 싶어? 좋아요, 화면의 오른쪽에 보이는 것은 내 장치입니다.
 지금은 작은 로봇 가게 응용 프로그램을 시작할 것입니다.
 그리고 내가 돌아 다니는 것을 볼 때 아무 일도 일어나지 않습니다.
 내가 잠시 멈추고 뭔가를 가리키면 노란 선이 보일 것입니다. 그리고 나서 짜잔이 나옵니다. 네 단계는 스테퍼 모터입니다.
 괜찮아? 우리가이 테이블에서 무엇을 더 가지고 있는지 보자. 저의 마이크로 컨트롤러입니다.
 그것은 스테퍼 모터 드라이버입니다.
 우리는 또한 당신이 알고있는 것을 좋아할 수 있습니다.
 예, 폐 루프 벨트입니다.
 우리가 여기 무엇을 가지고 있니? 리드 스크류.
 그리고 제가 말씀 드렸듯이, 제 케이블을 충분히 길게 들여다 보면 바코드를 볼 수 있습니다.
 그리고 그것이 나의 훈련 과정입니다.
 이봐, 프랭크? 물론, 그걸로 나는 프랭크? 무슨 일이야? Frank, 네, 데모에 다른 로봇 파트를 추가해야합니다.
 이쪽은 브렛입니다.
 내 매니저 야.
 그거 좋을거야.
 평소처럼 관리, 막판 요청.
 그 메모 사본 한 부를 더 얻도록하겠습니다.
 나는 이것을 위해 토요일에 가지 않을 것입니다.
 좋아, 그럼 우린 뭘 여기에 있니?


우리는 [들리지 않는] 모터를 가지고 있습니다.
 좋아, 보자.
 그것은 효과가있을 것입니다.
 내가 이것을 시도하자.
 나는 그걸로 벗어날 수 있습니까? 아니요,이 객체를 실제로 읽을 수는 없습니다.
 아마 그렇게? 그건 아니야, 스테퍼 모터가 아니야.
 그래서 그것은 버그입니다.
 나는 우리가 그것을 고칠 필요가 있다고 생각한다.
 누가 이것을 고치고 싶습니까? 누가 이것을 고치고 싶습니까? 좋구나.
 그래서, 지금해야 할 일은 앞서 말한, 들리지 않는 모터의 사진을 찍어야한다는 것입니다.
 그래서 스튜디오에 가야합니다. 조명을 설정하거나 이미 가지고있는 카메라를 사용합니다.
 자, 보자.
 우리는 우리의 [들리지 않는] 모터 사진을 잔뜩 찍을 것입니다.
 그리고 그것의 종류를 다양하게하는 것이 중요합니다. 프레임에 실제로 다른 것을 갖지 않는 것이 중요합니다.
 그래서 저는 [들리지 않음]으로 갈 것입니다.
 적어도 10 개의 서로 다른 이미지가 있어야합니다.
 좋은 선택.
 나는 항상 다른 배경 위에 놓기를 좋아합니다.
 그리고 우리가 여기서 몇 가지를 포착하도록하십시오.
 아마 나는 그것을 내 손에 조금 붙들 게 될 것이다.
 좋아요, 이제 우리는 많은 이미지를 가지고 있습니다.
 이제 Mac으로 가서 실제 교육을 수행하는 방법을 보여줄 것입니다.
 좋아, 나는 이미지 캡처 응용 프로그램을 가져오고 잠시 동안 내 [들리지 않음]을 숨기려고합니다.
 이제 Finder를 살펴보면, 필자는 필자가 이전에 훈련 및 모델로 사용했던 훈련 세트를 실제로 사용하고 있음을 볼 수 있습니다.
 이제 새 폴더를 만들어 서보를 불러 보겠습니다.
 이미지 캡쳐에서 방금 캡처 한 모든 사진을 가져 와서 내 서보로 끌 수 있습니다.
 좋아, 이제 우리는 그것을 추가했다.
 내 매니저가 이전에 해낸 일을 망쳤으니 이제 다시 모델을 훈련해야합니다.
 괜찮아.
 여기에 간단한 스크립팅 놀이터를 사용했습니다.
 UI가 아니기 때문에 나중에 애플리케이션에 빌드 단계로 통합하고자하는 내용 일 수 있습니다.
 그래서 우리가 방금 폴더를 추가 한 데이터 세트를 가리키고 있습니다. 분류기를 훈련 할 뿐이며 결국에는 내 모델을 작성합니다.
 이제 우리가 볼 수 있듯이 어떤 일이 벌어 질지, 우리는 종족들과 떨어져 있습니다.
 우리가 이미 폴더에 넣은 모든 이미지를 통과하고 거기에서 장면 인쇄물을 추출합니다.
 그것은 일어날 수밖에없는 모든 스케일링을 수행하고 결국에는이를 토대로 기차 및 모델링을 수행합니다.
 따라서 꽤 복잡한 작업이지만 실제로는 한 줄의 코드 일 뿐이며 결국에는 응용 프로그램에서 실제로 사용할 수있는 모델을 완성해야합니다.
 그냥 끝내자.
 거의 다 왔어.
 그리고 우리는 모델을 가지고 있습니다.
 이제, 그 모델은 이미 로봇 샵 애플리케이션에서 참조했습니다.
 이것은 우리가 지금 여기에서 보는 것입니다.
 보시다시피 내 이미지 분류 기준입니다.
 148 킬로바이트.
 그것은 내가 실제로 가지고있는 작은 시작 화면보다 작습니다.
 그래서 여기에서 이미 강조하고자하는 것이 있습니다. 그리고 나중에 조금 더 자세히 살펴 보도록하겠습니다.
 따라서이 이미지는 제가 전달해야하는 것입니다 - 컬러 이미지와 299 x 299 픽셀이어야합니다.
 이상한 레이아웃이지만 이것은 실제로 많은 분류 자들이 할 것입니다.
 좋구나.
 그래서, 지금은 그것을 이해할 수있는 모델을 가지고 있습니다.
 이제 핵심 목록 인 정교한 제품 데이터베이스에 들어가야합니다.
 그리고 저는 그것에 서보를 추가 할 것입니다.
 그래서 여기에이 이름의 이름을 바꿉니다.
 이것은 Servo입니다.
 나는 라벨을 붙이고있다.
 이것은 실제로 우리가 보게 될 것입니다.
 이 모터는 서보 모터이며 이것이 휙휙 움직이는 모터라고 가정 해 봅시다.
 매우 기술적입니다.
 좋구나.
 이것이 작동하는지 봅시다.
 이제 내 신청서를 실행할 것입니다.
 괜찮아.
 해 보자.
 우리의 서보 모터가 있습니다.
 이것을 관점에 넣기 만하면됩니다.
 이것은 당신이 처음 본 세계였습니다.
 스테이지부터 사진까지, 최종 응용 프로그램까지 모든 단계에서 연습 할 수있는 분류 자.
 나는이 데모에 대해 약간 땀을 흘렸다 [웃음].
 고맙습니다.
 이제 어떻게 작동하는지 살펴 보았습니다.
 실제로 코드를 살펴볼 때 실제로 강조하고 싶은 몇 가지 사항이 있습니다.
 그래서 저는 여기서 코드를 조금씩 살려고합니다.
 좋아, 지금 당장 볼 필요가없는 모든 것을 가져 가자.
 그리고 조금 더 크게 만드십시오.
 좋아, 그럼 내가 어떻게이 모든 걸 해결 했니? 그래서 우리는 실제로 Sequence Request Handler를 만들었습니다.
 세르게이가 이전 세션에서 이미 설명했듯이, 이것은 등록 작업에 사용할 것입니다. 이것은 개체 추적에 유용합니다.
 요청을 만들어 하나의 배열에 넣으 려합니다. 그러면 등록을 위해 여기에서 무엇을 보았습니까? 마지막 15 가지 등록 결과를 유지하는 것입니다. 그런 다음 그 분석을 수행하고 실제로 보유하고 있는지 확인합니다.
 나는 이것을 분석하는 동안 내가 붙잡고있는 버퍼 하나를 유지할 것입니다.
 이것은 실제로 분류를 실행할 때입니다.
 그리고 이것은 더 이상 실행되는 작업이 될 수 있으므로, 실제로 이것을 별도의 큐에서 실행할 것입니다.
 좋아, 여기에 내가 실제로 사용했던 코드가있다.

이것은 실제로 우리가 본 작은 패널과 같습니다.
 그러나 중요한 부분은 실제로 "어떻게 비전 작업을 설정합니까?"입니다. 그래서 저는 두 가지 일을 할 것입니다.
 바코드 요청을 할 것이고 분류 요청을 할 것입니다.
 그래서 바코드 요청을 설정했습니다.
 그리고 완성 작업 처리자에서 간단히 살펴 봅니다. "내가 뭔가 돌려 줄까요?" 그리고 내가 바코드 하나만을 기대하고 있기 때문에, 나는 아주 처음의 것을 보았습니다.
 그것을 디코딩 할 수 있습니까? 내가 문자열을 얻으면, 사실 그걸 사용하여 봅니다. 실제로 바코드와 함께 어떻게 작동하는지 보여줍니다. 오, 그래, 그건 내 훈련 [들리지 않음]입니다.
 알았어. 그래서 나는 그것을 실행하고 싶은 요청 중 하나로 추가한다.
 이제 분류를 설정하고 있습니다.
 그래서,이 경우, 내가 한 일은 제 분류자를 사용하고 제 묶음에서 단순히 이것을로드하고 거기에서 제 모델을 생성합니다.
 이제 Core ML의 코드 완성을 사용하지 않고 있습니다. Core ML의 유일한 라인이기 때문에 실제로 전체 응용 프로그램을 사용하고 있으며, 사용자 정의 종류의 오류 처리를 수행 할 수 있습니다.
 그러나 이미 코어 ML에서 코드 완성을 사용하도록 선택할 수도 있습니다.
 둘 다 절대적으로 유효합니다.
 이제부터 Vision 모델을 만듭니다.
 My Vision Core ML 모델 및 내 요청.
 그리고 다시 요청이 돌아 오면 완성 처리자를 실행하고 있습니다.
 간단히 살펴 보겠습니다. "어떤 사양으로 돌아 왔습니까?" 그리고 저는이 임계 값을 여기에 설정했습니다.
 이제, 이것은 내가 경험적으로 자신감 목표에 맞춰 설정 한 것입니다.
 나는 0을 사용하고있다.
98
 따라서 이것이 실제로 정확하다는 98 % 신뢰도.
 내가 왜 그랬어? 그게 내가 뭔가를보고있을 때 실제로 걸러 낼 수있게 해주고, 그게 뭔지 잘 모를 수도 있습니다.
 어쩌면 그 순간에 실제로 그 의미를 알 수있을 것입니다.
 그래서 지금, 나는 나의 모든 요구를 가지고있다.
 내가 실제로 그것을 실행하고 싶을 때가되면, 실제로 의미하는 작은 함수를 만들었습니다. "현재 이미지를 분석하십시오.
"분석 할 때가되면 장치 방향을 알게되었습니다. 이는 휴대 전화를 어떻게 들고 있는지를 아는 데 중요합니다.
 현재 처리하려는 버퍼에 이미지 요청 처리기를 만듭니다.
 비동기식으로 작업을 수행하게했습니다.
 코어 ML과 바코드 판독을 실제로 처리하기 위해서는 기본적으로해야 할 일만 남았습니다.
 자, 몇 가지, 그냥 괜찮아.
 장면 안정성 부분은 어떻게합니까? 따라서 대기열을 재설정 할 수 있습니다.
 나는 그것에 내 포인트를 추가 할 수 있습니다.
 그리고 간단하게 저는 제가 기록한 포인트 큐를 통해 기본적으로 볼 수있는 함수를 만들었습니다.
 그런 다음 "글쎄, 모두 합쳐도 20 픽셀 정도의 거리 만 보여줍니다.
"다시 한 번, 내가 선택한 경험적 가치입니다.
 그러면 그 장면이 안정적이라는 것을 압니다.
 그래서 나는 안정을 유지하고있다.
 아무것도 카메라 앞에서 움직이지 않습니다.
 그리고 출력물을 잡는 부분이 있습니다.
 따라서 실제로 AV Foundation에서 카메라의 버퍼를 호출하는 호출입니다.
 나는 내가 알고있는, 이전 픽셀 버퍼를 잡아 두는 것을 확실히하고있다. 왜냐하면 그것이 내가 등록한 것과 비교할 것이기 때문이며, 그 일을 끝내면 스왑한다.
 따라서 현재 버퍼로 번역 이미지 등록 요청을 만듭니다.
 그런 다음 Sequence Request Handler에서 간단히 요청합니다.
 이제 관측을 되 찾을 수 있습니다.
 그들이 모두 괜찮은지 확인할 수 있습니다.
 그리고 배열에 추가하십시오.
 그리고 마지막으로 중요한 것은 장면이 안정적인지 확인합니다.
 그런 다음 나는 감지 할 수있는 오버레이 인 작은 노란색 상자를 가져온다.
 이것이 현재 분석 된 버퍼라는 것을 압니다.
 그리고 단순히 그것에 대한 분석을 형성하도록 요청하십시오.
 내가 분석 한 버퍼에서 비동기 호출이 끝날 때 한 일은 내가 그 버퍼를 릴리스했음을 알았습니다.
 그리고 현재 사용중인 버퍼가 있는지 확인합니다.
 이제는 하나의 버퍼에서 실제로 작업하고 있는지를 확인할 수 있습니다. 그리고 배경에서 계속 실행되는 동안 점점 더 많은 버퍼를 큐에 넣지는 않습니다. 프레임에서 카메라가 굶어 죽을 것이기 때문입니다.
 좋습니다, 그래서 우리가 이것을 실행할 때, 실제로 강조하고 싶은 몇 가지가 있습니다.
 그래서, 번호 1, 우리의 콘솔을 조금 아래쪽으로 가져 오도록하겠습니다.
 그리고 제가 처음에 이것을 실행할 때 여러분은 실제로 이것을 볼 수 있습니다. 그래서 지금 여기에서 이것을 실행할 것입니다.
 잘하면 실제로 뭔가를 볼 수 있기를 바랍니다.
 당신은 [안 들음]보다 실제로 [들리지 않음] 때문에 자신감 점수가 상당히 낮다는 것을 알 수 있습니다.
 정말 내가 뭘보고 있는지 모르겠습니다.
 실제로 나는 그것이 식별해야하는 무엇인가를 지적합니다, 붐, 우리의 신뢰 점수는 정말로 높아집니다. 그리고 이것이 제가 실제로 보여주고 자하는 대상이라는 것을 실제로 확신 할 수 있습니다.
 자, 제가 데모를하고 싶었던 것이 있습니다. 실제로 CPU의 관점에서 어떤 일이 발생하는지 봅시다.
 알았어, 그래서 지금, 나는 아무것도하지 않을거야.
 나는 방금 내 화면을 보여주고있다.
 그래서 카메라를 움직일 때 장면이 안정적이지 않고 CPU의 약 22 %를 사용하고 있습니다.

이제 안정적으로 잡고 분류자를 실행하면 CPU가 어떻게 올라가는지 알 수 있습니다.
 그래서 필자는 항상 필요할 때만 이러한 작업을 실행하는 것이 좋습니다.
 알았어, 받아 들일 것이 많았다.
 슬라이드로 돌아가서 우리가 지금 보았던 것을 잠깐 살펴 보겠습니다.
 슬라이드 오른쪽으로 이동하십시오.
 알겠습니다.
 우선 장면 안정성을 어떻게 얻었습니까? 이전 프레임과 비교하기 위해 VN 번역 이미지 등록 요청과 함께 시퀀스 요청 처리기를 사용했습니다.
 그 중에서 우리는 이전 프레임이 어떻게 현재 프레임으로 이동했는지와 같은 X와 Y를 알려주는 정렬 변환의 용어로 변환을 얻습니다.
 그런 다음 장면이 안정적 일 때만 분석하기를 원한다고 이야기했습니다.
 이를 위해 VN Image Request Handler를 현재 버퍼에서 생성했습니다.
 그리고 우리는 바코드 감지와 분류를 함께 통과 시켰습니다.
 따라서 비전을 커버 아래에서 최적화 할 수 있으며 별도의 요청으로 실행하는 것보다 훨씬 빠르게 수행 할 수 있습니다.
 다음에 비행 중에 얼마나 많은 완충 대가 들어 있는지 생각하는 부분이 있었습니까? 이것이 내가 여러분의 버퍼를 관리하는 이유입니다.
 이러한 길쌈 네트워크와 같은 일부 비전 요청은 조금 더 오래 걸릴 수 있습니다.
 그리고 이러한 긴 실행 작업은 백그라운드 대기열에서 수행하는 것이 더 좋으므로 카메라에서 수행하는 작업을 실제로 계속 실행할 수 있습니다.
 그러나 특히 카메라로이를 수행하려면 카메라에서 나오는 버퍼를 계속 대기열에 넣고 싶지는 않습니다.
 그래서, 당신은 바쁜 버퍼를 드롭 싶어요.
 이 경우, 나는 오직 하나만 가지고 일한다고 말했다.
 실제로 유스 케이스 시나리오에서 잘 작동합니다.
 그래서 저는 1이라는 대기열을 가지고 있습니다. 그래서 그저 하나의 버퍼에 보관하고 확인하는 것입니다. 그 중 하나가 실행되고있는 한 새 버퍼를 대기 중이 아닙니다.
 일단 내가 끝내면 다음 버퍼를 재설정하고 작업 할 수 있습니다.
 이제, "Core ML에서이 모델을 실행할 수있을 때 Vision을 사용하는 이유는 무엇입니까? Core ML 모델입니다.
"글쎄, 이것이 비전을 실제로 사용하는 것이 중요한 이유가 하나 있습니다.
 다시 돌아가서 우리가 본 모형을 보았을 때 본 것을 봅시다.
 그것은 299 x 299 픽셀의 이상한 수입니다.
 자, 이것은 단순히이 모델이 어떻게 훈련되었는지입니다.
 이것은 그것이 섭취하기를 원하는 것입니다.
 그러나 우리 카메라는 원하는 경우 640x480 이상의 해상도를 제공합니다.
 이제 Vision은 카메라에서 나오는 버퍼를 가져 와서 RGB로 변환하고 크기를 줄이므로 코드를 작성할 필요가 없습니다.
 따라서 Vision을 통해 이미지 요청을 위해 Core ML 모델을 쉽게 구동 할 수 있습니다.
 그래서 이미지 분류였습니다.
 다음으로 우리는 물체 인식에 대해 이야기합니다.
 자, 약간의 경고.
 이 데모에서 실제 라이브 크로는 실제로 무대에 상처를 입힐 수 있습니다.
 그래서, 짜증나게하는 사람을 위해, 멀리 보아주세요.
 그래서, 우리가 우리의 물체 인식을 위해 사용하는 것은 YOLO 기법 인 You Only Look Once를 기반으로 한 모델입니다.
 그것은 우리가 객체와 그것의 레이블로부터 테두리 상자를 얻을 수있게 해주는 매우 빠른 실행 모델입니다.
 그리고 그것은 화면에서 그들 중 다수를 찾습니다.
 당신이 스크린 샷에서 보듯이.
 그 (것)들의 이점은 나가 실제로있는 곳에 좋아한다이다, 그러나 나는 우리가 전체적인 심상 분류기 같이 할 수있는 것처럼 많은 분류를 얻지 않을 것이다.
 트레이닝도 조금 더 복잡해졌습니다. 실제로, 저는 Turi Create 세션을 실제로보고 싶습니다. 어제, 그들이 실제로 이런 종류의 모델을 교육하는 방법을 보여주었습니다.
 이 모델들은 조금 더 큽니다.
 그래서, 어떻게 보이나요? 데모로 넘어 갑시다.
 로봇 샵이 폐쇄되었습니다.
 아침 먹을 시간이야.
 좋구나.
 여기에 빠른 템플릿을 가져오고 새로운 작은 애플리케이션이 생겼습니다.
 나의 아침 찾기입니다.
 그리고 우리는 무엇을 볼 수 있습니까? 오, 우리는 크루아상이 있고, 우리는 베이글을 가지고 있고 바나나를 확인할 수 있습니다.
 보시다시피, 그들은 모두 프레임과 같은 종류 일 수 있습니다.
 나는 그들을 발견 할 것이다.
 그래서,이 요리 쇼에서 어떤 언급, 일반적으로 당신에게 그것을 보여줄 방법을 보여 주지만, 그 다음 오븐에서 prebaked 물건을 꺼내.
 음,이 모델은 실제로이 빵을 굽기 전에 구워졌습니다. 그리고 저는 이것을 증명할 수 있습니다.
 신선 하네.
 그리고 여전히 크로와상.
 좋구나.
 신선하지만 여전히 씹어 야합니다.
 이 코드에서 어떻게 보이는지 빠르게 살펴 보겠습니다.
 그래서, 내가 어떻게 다르게 했니? 실제로 내 요청을 설정하는 것과 같은 측면에서 [들리지 않음].
 내가해야 할 일은 이전 예제에서했던 것처럼 핵심 ML 모델을 사용하는 것입니다. 핵심 ML 요청을 작성한 후 실제로 간단하게 "어떻게 결과를 그릴 수 있습니까?" 이제 이것이 우리가 이것을 좀더 쉽게 이해할 수있는 새로운 것이 있습니다.
 그리고이 모든 것을 보았을 때 인식 할 수없는 객체 관찰 인 새로운 객체를 얻었습니다.이 객체에서 내 테두리 상자를 얻었고 라벨과 같이 관찰했습니다.

자, 여기 당신에게 보여주고 싶은 것이 하나 있습니다.
 여기에서 우리의 응용 프로그램을 실행하고 중단 점을 넣습니다.
 괜찮아.
 좋아, 우리는 지금 우리의 브레이크 포인트에있다.
 그래서 저는 첫 번째 레이블 만 봅니다.
 그래서 우리가 실제로이 결과를 처리 할 때 우리가하고있는 일은 제가 이것을 취하고 객체 관찰, 레이블을 사용해 봅시다.
 그래서, 실제로 보시는 것은 제가 둘 이상을 얻는 것입니다.
 나는 나의 베이글, 나의 바나나, 커피를 얻는다 - 나는 어떤 커피도 오늘 가지고 오지 않았다.
 미안합니다.
 그리고 크로와상, 계란, 와플.
 이제 그들은 최상위 신뢰도와 같은 순서로 정렬됩니다.
 일반적으로 관심있는 내용입니다.
 그래서 내가 여기에 바로 가기를 가져 가고 첫 번째 것을보고있는 것입니다.
 그러나 당신은 항상 우리가 모델에서 실제로 지원하는 배열의 관점에서 모든 분류를 되돌려 놓습니다.
 좋구나.
 그것은 우리의 Breakfast Finder였습니다.
 슬라이드로 돌아가서 이번에는 버튼을 밀고 있습니다.
 좋은.
 그래서 우리는 이것을 새로운 API를 통해 가능하게 만들었습니다. 그것은 우리의 VN Recognized Object Observation입니다.
 코어 ML 모델 요청을 수행 할 때 자동으로 실행되며 모델에서 실제로 개체 감지기를 공간으로 사용하는 경우 자동으로 실행됩니다.
 이 예와 같이 YOLO 기반 모델을 기반으로합니다.
 이제, "글쎄, 나는 지난해처럼 이미 YOLO를 운영했을 수있다.
 웹에서 본 수많은 기사가있었습니다.
"그러나 실제로이 모델의 산출물을 얻기 위해 작성해야하는 코드의 양을 살펴본 다음 사용할 수있는 코드에 넣으십시오.
 여기에는 몇 줄의 코드 만 있습니다.
 따라서 YOLO 모델을 실제로 사용하기가 정말 쉽습니다.
 이 코드를 다시 한 번 살펴 보겠습니다.
 그래서, 제 모델을 만듭니다.
 모델에서 내 요청을 만듭니다.
 그리고 완성 처리기에서 여러 객체를 다시 얻을 수 있기 때문에 객체 영역을 볼 수 있습니다.
 내 라벨을 내 경계 상자에서 가져 와서 내 아침 찾기 Finder를 표시 할 수 있습니다.
 이제이 예제에서 강조하고 싶은 또 하나의 것이 있습니다.
 당신은 감지기 프레임을 프레임 단위로, 프레임 단위로, 프레임 단위로 실행했기 때문에이 상자들이 약간 엉망이되는 것을 보았습니다.
 추적이 더 나은 선택 일 수 있습니다.
 왜? 추적은 달리기와 같은 측면에서도 훨씬 빠릅니다. 실제로이 모델이 실행될 때보 다 훨씬 빠릅니다.
 따라서 추적을 실제로 실행하는 것보다 재검사에 더 많은 시간이 걸립니다.
 나는 더 가벼운 알고리즘이기 때문에 기본적으로 화면의 객체를 따라 가고 싶다면 기본적으로 추적기를 사용할 수 있습니다.
 그것은 더 빨리 달린다.
 그리고 그 위에는 시간적 평활화 기능이 있으므로이 상자는 더 이상 지터가 발생하지 않으며 일부 추적 사례를 볼 경우 화면에서 실제로 부드럽게 움직입니다.
 추적에 대해 더 자세히 알고 싶다면 동료 Sergei의 이전 세션에서 실제로 모든 구현 작업을 수행하는 방법에 대해 이야기합니다.
 좋아, 마지막으로, 우리의 비전 숙달을 강화하고 우리의 기본 사항 중 일부로 들어가 보겠습니다.
 Vision 프레임 워크를 다룰 때 알아야 할 중요한 사항은 거의 없습니다.
 맨 먼저, 그리고 이것은 문제의 일반적인 원인, 이미지 방향입니다.
 이제 모든 Vision 알고리즘이 오리엔테이션에 독립적 인 것은 아닙니다.
 우리는 오리엔테이션에 불가지론자인 새로운 얼굴 탐지기가 있다는 것을 일찍 듣게되었을 것입니다.
 그러나 이전의 것은 그렇지 않았습니다.
 이것은 우리가 이미지의 직립 위치가 무엇인지 알아야한다는 것을 의미합니까? 당신이 그것을보고 마지막에 미리보기를하면 이미지가 똑바로 보일 수 있지만 그것이 디스크에 저장되는 방식이 아니기 때문에 속일 수 있습니다.
 장치가 어떻게 지향되는지 알려주는 무언가가 있습니다. 이것을 EXIF ​​오리엔테이션이라고합니다.
 따라서 이미지가 캡처되면 일반적으로 센서 방향으로, EXIF를 사용하면 실제로 무엇이 올바른지 알게되고, 입력으로 Vision에 URL을 전달하면 Vision은 실제로 기본적으로 작동하는 모든 작업을 수행합니다. 실제로 파일에서이 EXIF ​​정보를 읽으십시오.
 그러나 이전에 데모에서 보여 주었던 것처럼 라이브 캡처 피드를 사용하면 실제로이 정보를 전달해야합니다.
 그래서, 저는 우리 UI 장치에서 내 방향이 현재 방향이고,이 이미지를 EXIF ​​방향의 형태로 필요하기 때문에이 [들리지 않음]을 CG Image Property Orientation으로 변환해야합니까?
 다음으로 좌표계에 대해 조금 이야기 해 봅시다.
 비전의 경우 출발점은 항상 왼쪽 하단에 있습니다.
 그리고 모든 처리가 오른쪽 상단에서 수행됩니다. 이미지가 수직 위치에 있으면 방향이 중요합니다.
 우리의 모든 처리는 정규화 된 좌표 공간에서 실제로 수행됩니다. 등록을 제외하고 실제로 얼마나 많은 픽셀 [알아들을 수없는]을 알아야합니다.
 따라서 정규화 된 좌표는 오른쪽 위 모서리에서 좌표가 0, 0에서 1,1로 이동한다는 것을 의미합니다.
 이제 여기에서 보는 것은 얼굴과 표식 탐지 요청을 수행하는 것입니다.
 그리고 내 얼굴에 테두리 상자가 생기고 랜드 마크가 실제로 해당 경계 상자에 상대 좌표로보고됩니다.

이미지 좌표 공간으로 되돌아 가야한다면 VN 이미지와 같은 유틸리티 함수와 VNUtils가 정상적인 방법으로 앞뒤로 좌표를 변환 할 수 있습니다.
 다음으로 신뢰 점수에 대해 이야기 해 봅시다.
 우리는 로봇 샷 예제에서 이미이 부분을 조금 만진 적이 있습니다.
 Google의 알고리즘 중 상당수가 결과에 자신감을 표현할 수 있습니다.
 그리고 그것은 제가 나중에 이러한 결과에서 벗어나서 분석 할 때를 아는 중요한 부분입니다.
 그래서, 만약 내가 0의 낮은 확신을 가지고 있거나, 1의 높은 확신을 가지고 있습니까? 식자 과장.
 여기에 우리가 간다.
 좋구나.
 불행히도 모든 알고리즘이 자신의 신뢰 점수를보고하는 방식과 같은 척도를 갖지는 않습니다.
 예를 들어, 텍스트 감지기를 보면 텍스트가 있다고 생각하지 않으면 처음에는 경계 상자를 반환하지 않으므로 신뢰도 점수는 1을 반환합니다.
 그러나 우리가 보았 듯이, 분류 자들은이 신뢰 점수가 될 수있는 것의 실제 범위가 매우 넓습니다.
 몇 가지 예를 살펴 보겠습니다.
 첫 번째 예에서는 로봇 상점 예제의 이미지를 사용하고 모델을 직접 실행했습니다.
 그리고 확실히, 그것은 매우 높은 신뢰를 가지고있었습니다. 이것은 스테퍼 모터입니다.
 이제 다음 예제에서 우리 모델 갤러리에있는 모델 중 일부를 사용할 것입니다.
 그래서, 나를 틀리게하지 마십시오.
 모델의 품질을 비교하고 싶지 않습니다.
 실제적으로 그들이 무엇을 반환하고 자신이 실제로 무엇을하고 싶은지에 관한 것입니다.
 그렇다면이 이미지를 분류 할 때 기본적으로 무엇이 우리에게 무엇을 말했습니까? 음, 그렇게 나쁘지는 않지만 실제로는 확실합니다.
 신뢰 지수는 0입니다.
395는 특별히 높지는 않지만 그렇습니다. 모래 부분이 있습니다. 해변이 있습니다.
 따라서 기본적으로 검색 할 때 사용할 수 있지만 그 이미지에 레이블을 지정할 수 있습니까? 아마 의심 스럽네.
 다음 예제를 살펴 보겠습니다.
 스쿠터에 소녀입니다.
 분류기는 이것을 어떻게 처리 했습니까? 글쎄, 나는 그녀가 고구마라고 불릴만큼 행복하다고 확신하지 못한다.
 한 가지 더 예를 살펴 보겠습니다.
 그래서, 여기 내 코드의 스크린 샷입니다.
 분류 자의 역할은 무엇입니까? 웹 사이트라고 생각합니다.
 컴퓨터는 너무 바보입니다.
 그래서 우리의 신뢰 점수에 대한 결론이 나왔습니다.
 1 않습니다.
0은 항상 100 % 정확함을 의미합니까? 꼭 그런 것은 아닙니다.
 그것은 알고리즘의 기준을 채울 것이지만, 고구마와 특히봤을 때 우리의 인식은 상당히 다릅니다.
 따라서,이를 활용하고자하는 응용 프로그램을 만들 때 염두에 두십시오.
 생각 해봐.
 의료 응용 프로그램을 작성하고 "아, 암에 걸렸다"고 말하면 결과에 실제로 얼마나 확신 할 수 있는지에 따라 조금 부드럽게하려는 매우 강력한 주장 일 수 있습니다.
 그래서 이것을 위해 사용할 수있는 두 가지 기술이 있습니다.
 로봇 샷 예제에서 보았 듯이 신뢰도 점수에 임계점을 사용했습니다. 실제로 이미지에 라벨을 지정했기 때문에 실제로 신뢰도가 낮은 모든 것을 필터링 할 때 알았습니다.
 반면에 검색 응용 프로그램을 만들려는 경우 실제로 검색 결과에 유효한 선택 사항이 남아 있기 때문에 실제로 검색의 맨 아래에있는 이미지 중 일부를 사용하여 보여줄 수도 있습니다.
 평소와 같이, 우리는 우리 웹 사이트에서 더 많은 정보를 찾습니다.
 그리고 우리는 내일 3PM에서 실험실을 갖습니다.
 들러주세요.
 질문하십시오.
 우리는 당신을 도울 것입니다.
 그리고 그 첫 번째로 우리 기술로 창조 한 위대한 응용 프로그램에 대해 모두에게 감사드립니다.
 나는 당신이 이것으로 무엇을 할 수 있는지보기를 고대하고 있습니다.
 WWDC에 오신 것을 모두 감사드립니다.
 쇼의 대단한 휴식을 가져라.
 고맙습니다.
